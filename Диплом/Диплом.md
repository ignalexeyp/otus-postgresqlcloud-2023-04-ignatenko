# Кластера Patroni

# Цель:
  Развернуть Отказоустойчивый кластер Postgresql на основе  patroni+etcd+pgbouncer+haproxy+keepalived в 
  различных конфигурациях.
  Протестировать отказоустойчивость кластера.
  Настроить резервное копирование и восстановление кластера с использованием pg_probackup.


# Развертывание кластера Postgresql на основе  patroni+etcd+pgbouncer+haproxy+keepalived в максимальной конфигурации 

   Имеем 8 виртуальных машин (Microsoft Hyper-V) ubuntu 22.04.5 LTS server с SSH , но без обновлений 
   vm-postgresql-dev-5			ip 10.128.3.195 
   vm-postgresql-dev-6			ip 10.128.3.196 
   vm-postgresql-dev-7			ip 10.128.3.197
   vm-postgresql-dev-8			ip 10.128.3.198
   vm-postgresql-dev-9			ip 10.128.3.199
   vm-postgresql-dev-10			ip 10.128.3.200
   vm-postgresql-dev-11.prod.ru	        ip 10.128.3.169
   vm-postgresql-dev-12.prod.ru	        ip 10.128.3.170

   На машинах vm-postgresql-dev-5 vm-postgresql-dev-6 vm-postgresql-dev-7 устанавливаю etcd
   На машинах vm-postgresql-dev-8 vm-postgresql-dev-9 vm-postgresql-dev-10 устанавливаю PostgreSql, здесь же будут размещаться базы данных.
   Кроме этого на этих машинах устанавливается patroni и pgbouncer.
   На машинах vm-postgresql-dev-11.prod.ru vm-postgresql-dev-12.prod.ru устанавливаю HAProxy и keepalived.

   vm-postgresql-dev-5, vm-postgresql-dev-6 Количество процессоров: 1  Объем дискового пространства (GB): 40  Объем оперативной памяти (GB): 2
   Остальные ВМ Количество процессоров: 2  Объем дискового пространства (GB): 80  Объем оперативной памяти (GB): 2

### УСТАНОВКА ETCD


  Обновиляю локальный индекс пакетов сервера. На всех нодах выполняю: 

   sudo apt-get update; apt upgrade -y 
   sudo apt install -y etcd  

  Устанавливаю etcd

   for i in {5,6,7}; do  ssh vm-postgresql-dev-$i --command='sudo apt update && sudo apt upgrade -y && sudo apt install -y etcd' & done;

  Проверяю, что c etcd.

   for i in vm-postgresql-dev-{5,6,7}; do ssh ${i} 'hostname; ps -aef | grep etcd | grep -v grep'; done 


  Останавливаю сервисы etcd

   for i in vm-postgresql-dev-{5,6,7}; do ssh ${i} 'sudo systemctl stop etcd'; done 

  Добавляю в файлы с конфигами /etc/default/etcd. При работающем DNS можно прописывать имена инстасов, иначе IP адреса.

   (ETCD_INITIAL_CLUSTER="vm-postgresql-dev-5=http://vm-postgresql-dev-5:2380,vm-postgresql-dev-6=http://vm-postgresql-dev-6:2380,vm-postgresql-dev-7=http://vm-postgresql-dev-7:2380")
  
   cделал 

   (ETCD_INITIAL_CLUSTER="vm-postgresql-dev-5=http://10.128.3.195:2380,vm-postgresql-dev-6=http://10.128.3.196:2380,vm-postgresql-dev-7=http://10.128.3.197:2380")

  Я прописывал IP адреса, хотя DNS работает. Посмотреть DNS сервер cat /etc/resolv.conf.

  Инстанс vm-postgresql-dev-5

   sudo  nano  /etc/default/etcd

  Добавляю

   ETCD_NAME="vm-postgresql-dev-5"
   ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
   ETCD_ADVERTISE_CLIENT_URLS="http://10.128.3.195:2379"
   ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
   ETCD_INITIAL_ADVERTISE_PEER_URLS="10.128.3.195:2380"
   ETCD_INITIAL_CLUSTER_TOKEN="STPatroniCluster"
   ETCD_INITIAL_CLUSTER="vm-postgresql-dev-5=http://10.128.3.195:2380,vm-postgresql-dev-6=http://10.128.3.196:2380,vm-postgresql-dev-7=http://10.128.3.197:2380"
   ETCD_INITIAL_CLUSTER_STATE="new"
   ETCD_DATA_DIR="/var/lib/etcd"

  Инстанс vm-postgresql-dev-6

   sudo  nano  /etc/default/etcd

   ETCD_NAME="vm-postgresql-dev-6"
   ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
   ETCD_ADVERTISE_CLIENT_URLS="http://10.128.3.196:2379"
   ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
   ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.128.3.196:2380"
   ETCD_INITIAL_CLUSTER_TOKEN="STPatroniCluster"
   ETCD_INITIAL_CLUSTER="vm-postgresql-dev-5=http://10.128.3.195:2380,vm-postgresql-dev-6=http://10.128.3.196:2380,vm-postgresql-dev-7=http://10.128.3.197:2380"
   ETCD_INITIAL_CLUSTER_STATE="new"
   ETCD_DATA_DIR="/var/lib/etcd"

  Инстанс vm-postgresql-dev-6

   sudo  nano  /etc/default/etcd

   ETCD_NAME="vm-postgresql-dev-7"
   ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
   ETCD_ADVERTISE_CLIENT_URLS="http://10.128.3.197:2379"
   ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
   ETCD_INITIAL_ADVERTISE_PEER_URLS="10.128.3.197:2380"
   ETCD_INITIAL_CLUSTER_TOKEN="STPatroniCluster"
   ETCD_INITIAL_CLUSTER="vm-postgresql-dev-5=http://10.128.3.195:2380,vm-postgresql-dev-6=http://10.128.3.196:2380,vm-postgresql-dev-7=http://10.128.3.197:2380"
   ETCD_INITIAL_CLUSTER_STATE="new"
   ETCD_DATA_DIR="/var/lib/etcd"

  Старт на всех трех инстансах (vm-postgresql-dev-5 vm-postgresql-dev-6 vm-postgresql-dev-7)

   for i in vm-postgresql-dev-{5,6,7}; do ssh ${i} 'sudo systemctl start etcd'; done

  Проверка автозагрузки на каждом инстансе

   systemctl is-enabled etcd

  Проверка etcd-кластера. Запускаем на одном инстансе etcd-кластера. 

   etcdctl cluster-health

### УСТАНОВКА POSTGRESQL

  Устанавливаю PostgreSql на инстансы vm-postgresql-dev-8 vm-postgresql-dev-9 vm-postgresql-dev-10

for i in vm-postgresql-dev-{8,9,10}; do ssh ${i} 'sudo apt update && sudo apt upgrade -y -q && echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" | sudo tee -a /etc/apt/sources.list.d/pgdg.list && wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - && sudo apt-get update && sudo apt -y install postgresql-14'; done


  Убеждаюсь, что кластера Postgresql стартовали

   for i in vm-postgresql-dev-{8,9,10}; do ssh ${i} 'hostname; pg_lsclusters'; done

### УСТАНОВКА PATRONI

  Устанавливаю python на все инстансы с Postgresql 

   sudo apt-get install -y python3 python3-pip git mc
   sudo pip3 install psycopg2-binary 

  После установки ПО останавливаем и удаляем экземлпяр постгреса который запускается по-умолчанию

   sudo -u postgres pg_ctlcluster 14 main stop
   --sudo systemctl stop postgresql@14-main
   sudo -u postgres pg_dropcluster 14 main 

  убеждемся что их нет

   sudo pg_lsclusters

  устанавливаем патрони 

   sudo pip3 install patroni[etcd]

  делаем симлинк

   sudo ln -s /usr/local/bin/patroni /bin/patroni

  включаем старт сервиса
  для каждого инстанса с patroni вносим информацию в patroni.service (пример в файле patroni.service)

   sudo nano /etc/systemd/system/patroni.service

   [Unit]
   Description=High availability PostgreSQL Cluster
   After=syslog.target network.target

   [Service]
   Type=simple
   User=postgres
   Group=postgres
   ExecStart=/usr/local/bin/patroni /etc/patroni.yml
   KillMode=process
   TimeoutSec=30
   Restart=no

   [Install]
   WantedBy=multi-user.target


  для каждого инстанса с patroni вносим информацию в /etc/patroni.yml шаблон один но надо проставить имена и хосты для каждой ноды свои 
  пример для 10.128.3.198

   sudo nano /etc/patroni.yml


   scope: patroni
   name: vm-postgresql-dev-8

   restapi:
     listen: 10.128.3.198:8008
     connect_address: 10.128.3.198:8008
   etcd:
    hosts: vm-postgresql-dev-5:2379,vm-postgresql-dev-6:2379,vm-postgresql-dev-7:2379
   bootstrap:
     dcs:
       ttl: 30
       loop_wait: 10
       retry_timeout: 10
       maximum_lag_on_failover: 1048576
       postgresql:
         use_pg_rewind: true
         parameters:
     initdb:
     - encoding: UTF8
     - data-checksums
     pg_hba:
     - host replication replicator 10.128.0.0/8 md5
     - host all all 10.128.0.0/8 md5
     users:
       admin:
         password: admin321
         options:
           - createrole
           - createdb
   postgresql:
     listen: 127.0.0.1, 10.128.3.198:5432
     connect_address: 10.128.3.198:5432
     data_dir: /var/lib/postgresql/14/main
     bin_dir: /usr/lib/postgresql/14/bin
     pgpass: /tmp/pgpass0
     authentication:
       replication:
         username: replicator
         password: reppass321
       superuser:
         username: postgres
         password: zalando321
       rewind:  # Has no effect on postgres 10 and lower
         username: rewinduser
         password: rewindpassword321
     parameters:
       unix_socket_directories: '.'
   tags:
       nofailover: false
       noloadbalance: false
       clonefrom: false
       nosync: false

  запускаем patroni

   sudo -u postgres patroni /etc/patroni.yml

   sudo systemctl is-enabled patroni 
   sudo systemctl enable patroni 
   sudo systemctl start patroni 
   sudo systemctl stop patroni 
   sudo patronictl -c /etc/patroni.yml list 
   sudo systemctl status patroni 

 
  В зависимости от нод меняется:

   listen: 127.0.0.1, 10.128.3.198:5432
   connect_address: 10.128.3.198:5432

  посмотреть состояние patrony

   sudo patronictl -c /etc/patroni.yml list 


### pgbouncer

   for i in vm-postgresql-dev-{8,9,10}; do ssh ${i} 'sudo apt install -y pgbouncer'; done

  Создал базу chicago  

   create database chicago;

   sudo nano /etc/pgbouncer/pgbouncer.ini

  Вставляем в /etc/pgbouncer/pgbouncer.ini


[databases]
chicago = host=127.0.0.1 port=5432 dbname=chicago 
[pgbouncer]
logfile = /var/log/postgresql/pgbouncer.log
pidfile = /var/run/postgresql/pgbouncer.pid
listen_addr = *
listen_port = 6432
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt
admin_users = postgres

  Если необходим доступ к нескольким базам данным, добавляем например

   testuser = host=127.0.0.1 port=5432 dbname=testuser  



   !!! Имелась проблема с подключением к БД со стороны клиента (pgAdmin4, DataGrip). 
   !!! Необходимо раскомментарить параметр ;;ignore_startup_parameters = extra_float_digits в /etc/pgbouncer/pgbouncer.ini

  Установил  net-tools посмотрим процессы

   sudo apt install net-tools
   netstat -pltn

   sudo systemctl status pgbouncer 
   sudo systemctl stop pgbouncer 
   sudo systemctl start pgbouncer
 

   sudo -u postgres psql -h localhost
   create user admindb with password 'root123';
   select * from users;

  \du
 - list role

  Необходимо внести информацию в /etc/pgbouncer/userlist.txt  о пользователях, подключающихся через pgbouncer

   sudo -u postgres psql -h localhost
   postgres=# select usename,passwd from pg_shadow;

     usename   |                                                                passwd
   ------------+---------------------------------------------------------------------------------------------------------------------------------------
    postgres   | SCRAM-SHA-256$4096:Z7I9sYPjjLJiDGlEa2NGjg==$Xz1MZAcKJCF+dxJhRN9WcHaBt2WnsrLUa1PbwxVpiCU=:YmNHdjAARNXVYzeuEkRlHMPlc8LJsNTUXmcERVHz9y8=
    replicator | SCRAM-SHA-256$4096:Cky8wsNlEDSjU+qAq3jlpg==$RKaEDaCCkyrGehhf48U98SAM6y607LrtueJPSI05hdM=:IYLeoy1Y7SLIME5eQCXltBNefv8dk15gyoz0pO/i1VQ=
    rewinduser | SCRAM-SHA-256$4096:ulschQClUbXOT8rvWxY85A==$bAfAcS+QPH0IPY47AuWzbk6O6C2u/963l4RZVCBZYE4=:96nxtnoFTPCokdJ9H/eT8gjyD1q2NZpQW2ojGCw05GA=
    admin      | SCRAM-SHA-256$4096:x02YbYPWCcyTnaXVQOCOSA==$3gQyFZ2ox6DKrWNS7pCDn/nzrEdaPPQ0BKVCoMcTq9g=:pRt249q2M/L2sqQYVDS+rgY8YMeTnF0KZCkaVReHoHo=
    admindb    | SCRAM-SHA-256$4096:fJOMTtmAfIRaG9PPFkaLkQ==$1tlGA6bXc+At8+1bC7JPxjWcKkjV4R1ScUg6/DjC3PM=:d3suEa2k+K4n8qLMEtH8toQLXy47RwdZ1LgW9/norZg=
    testuser   | SCRAM-SHA-256$4096:g0UGgROg54Ifbtv9eaOWpQ==$Fh6t0lwQJiDGd66EPDKmm7korJlUlv5c0zoIXuYaT6k=:sj4aLUYazRLJRP1dDltc36o6YQRISzI3r6GBQlDxZ74=

   sudo nano /etc/pgbouncer/userlist.txt

   "postgres"  "SCRAM-SHA-256$4096:Z7I9sYPjjLJiDGlEa2NGjg==$Xz1MZAcKJCF+dxJhRN9WcHaBt2WnsrLUa1PbwxVpiCU=:YmNHdjAARNXVYzeuEkRlHMPlc8LJsNTUXmcERVHz9y8="
   "testuser"  "SCRAM-SHA-256$4096:g0UGgROg54Ifbtv9eaOWpQ==$Fh6t0lwQJiDGd66EPDKmm7korJlUlv5c0zoIXuYaT6k=:sj4aLUYazRLJRP1dDltc36o6YQRISzI3r6GBQlDxZ74="

  рестарт pg_bouncer

   sudo systemctl restart pgbouncer 

  При падении сам стартует

   sudo nano /lib/systemd/system/pgbouncer.service
   Restart=always


### Установка HAPROXY


  Выполнил на нодах vm-postgresql-dev-11.prod.ru vm-postgresql-dev-12.prod.ru

   sudo apt install -y --no-install-recommends software-properties-common && sudo add-apt-repository -y ppa:vbernat/haproxy-2.5 && sudo apt install -y haproxy=2.5.\*

  Проверил состояния на нодах haproxy

   curl -v 10.128.3.200:8008/master

  Настроим /etc/haproxy/haproxy.cfg.

   sudo nano /etc/haproxy/haproxy.cfg


   listen postgres_write
       bind *:5432
       mode            tcp
       option httpchk
       http-check connect
       http-check send meth GET uri /master
       http-check expect status 200
       default-server inter 10s fall 3 rise 3 on-marked-down shutdown-sessions
       server vm-postgresql-dev-8 10.128.3.198:6432 check port 8008
       server vm-postgresql-dev-9 10.128.3.199:6432 check port 8008
       server vm-postgresql-dev-10 10.128.3.200:6432 check port 8008

   listen postgres_read
       bind *:5433
       mode            tcp
       http-check connect
       http-check send meth GET uri /replica
       http-check expect status 200
       default-server inter 10s fall 3 rise 3 on-marked-down shutdown-sessions
       server vm-postgresql-dev-8 10.128.3.198:6432 check port 8008
       server vm-postgresql-dev-9 10.128.3.199:6432 check port 8008
       server vm-postgresql-dev-10 10.128.3.200:6432 check port 8008

  Выполняем рестарт haproxy.service и смотрим статус

   sudo systemctl restart haproxy.service
   sudo systemctl status haproxy.service

  Смотрим лог

   sudo cat /var/log/haproxy.log


### Настройка keepalived на хостах с HAproxy


  HAproxy устанавливается на нодах vm-postgresql-dev-11.prod.ru vm-postgresql-dev-12.prod.ru

   sudo apt install -y keepalived

  Включаем виртуальный сетевой коммутатор

   sudo nano /etc/sysctl.conf

  дописываем

   net.ipv4.ip_nonlocal_bind=1

   sudo sysctl -p

  посмотрим на какой интерфейс нужно добавить виртуальный ip

   ip a

   sudo nano /etc/keepalived/keepalived.conf

   global_defs {
   # Keepalived process identifier
   lvs_id haproxy_DH
   }
   # Script used to check if HAProxy is running
   vrrp_script check_haproxy {
   script "killall -0 haproxy"
   interval 2
   weight 2
   }
   # Virtual interface
   # The priority specifies the order in which the assigned interface to take over in a failover
   vrrp_instance VI_01 {
   state MASTER
   interface eth0
   virtual_router_id 51
   priority 101
   # The virtual ip address shared between the two loadbalancers
   virtual_ipaddress {
   10.128.3.230
   }
   track_script {
   check_haproxy
   }
   }

  sudo service keepalived start

  посмотрим успешность добавления IP

   ip a

# Развертывание кластера Postgresql на основе  patroni+etcd+pgbouncer+haproxy+keepalived в конфигурации lite
# !!! используя единый DCS

  vm-postgresql-1 10.128.3.134 PATRONI, POSTGRESQL, pgbouncer
  vm-postgresql-3 10.128.3.179 PATRONI, POSTGRESQL, pgbounce 
  vm-postgresql-4 10.128.3.183 HAPROXY, keepalived

  Используется общий dcs(etcd) c "основным" кластером
  vm-postgresql-dev-5 ip 10.128.3.195 
  vm-postgresql-dev-6 ip 10.128.3.196 
  vm-postgresql-dev-7 ip 10.128.3.197

### Установка POSTGRESQL

  Устанавливаю PostgreSql на инстансы vm-postgresql-1 10.128.3.134 vm-postgresql-3.systtech.ru 10.128.3.179 

  sudo apt update && sudo apt upgrade -y -q && echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" | sudo tee -a /etc/apt/sources.list.d/pgdg.list && wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - && sudo apt-get update && sudo apt -y install postgresql-15
  --на каждом инстансе

  --Убеждаюсь, что кластера Postgresql стартовали
  for i in vm-postgresql-{1,3}; do ssh ${i} 'hostname; pg_lsclusters'; done
  --выполняется на любом из инстансов с Postgresql


### Установка PATRONI

  --Устанавливаю python на все инстансы с Postgresql 

  sudo apt-get install -y python3 python3-pip git mc
  sudo pip3 install psycopg2-binary 

  -- после установки ПО останавливаем и удаляем экземлпяр постгреса который запускается по-умолчанию:
  -- sudo -u postgres pg_ctlcluster 15 main stop
  sudo systemctl stop postgresql@15-main
  sudo -u postgres pg_dropcluster 15 main 

  -- убеждемся что их нет
  sudo pg_lsclusters

  -- устанавливаем патрони 
  sudo pip3 install patroni[etcd]

  -- делаем симлинк
  sudo ln -s /usr/local/bin/patroni /bin/patroni

  -- включаем старт сервиса
  -- для каждого инстанса с patroni вносим информацию в patroni.service 
  sudo nano /etc/systemd/system/patroni.service
  -- Добавляем в него
  -----------------------
  [Unit]
  Description=High availability PostgreSQL Cluster
  After=syslog.target network.target

  [Service]
  Type=simple
  User=postgres
  Group=postgres
  ExecStart=/usr/local/bin/patroni /etc/patroni.yml
  KillMode=process
  TimeoutSec=30
  Restart=no

  [Install]
  WantedBy=multi-user.target
  ------------------------

  -- для каждого инстанса с patroni вносим информацию в /etc/patroni.yml шаблон один но надо проставить имена и хосты для каждой ноды свои 
  -- пример для 10.128.3..134

  sudo nano /etc/patroni.yml
  -----------------------------------------------------------
  
  scope: patroni
  name: patroni-1

  restapi:
    listen: 10.128.3.134:8008
    connect_address: 10.128.3.134:8008
  etcd:
    hosts: vm-postgresql-5:2379,vm-postgresql-6:2379,vm-postgresql-7:2379
  bootstrap:
    dcs:
      ttl: 30
      loop_wait: 10
      retry_timeout: 10
      maximum_lag_on_failover: 1048576
      postgresql:
        use_pg_rewind: true
        parameters:
    initdb:
    - encoding: UTF8
    - data-checksums
    pg_hba:
    - host replication replicator 192.168.0.0/8 md5
    - host all all 192.168.0.0/8 md5
    users:
      admin:
        password: admin321
        options:
          - createrole
          - createdb
  postgresql:
    listen: 127.0.0.1, 10.128.3.134:5432
    connect_address: 10.128.3.134:5432
    data_dir: /var/lib/postgresql/15/main
    bin_dir: /usr/lib/postgresql/15/bin
    pgpass: /tmp/pgpass0
    authentication:
      replication:
        username: replicator
        password: reppass321
      superuser:
        username: postgres
        password: zalando321
      rewind:  # Has no effect on postgres 10 and lower
        username: rewinduser
        password: rewindpassword321
    parameters:
      unix_socket_directories: '.'
  tags:
      nofailover: false
      noloadbalance: false
      clonefrom: false
      nosync: false

  --------------------------------------------------------------

  -- пример для 10.128.3.179
  sudo nano /etc/patroni.yml
  -- Добавляем в него
  ---------------------------
  scope: patroni
  name: patroni-2

  restapi:
    listen: 10.128.3.179:8008
    connect_address: 10.128.3.179:8008
  etcd:
   hosts: vm-postgresql-5:2379,vm-postgresql-6:2379,vm-postgresql-7:2379
  bootstrap:
    dcs:
      ttl: 30
      loop_wait: 10
      retry_timeout: 10
      maximum_lag_on_failover: 1048576
      postgresql:
        use_pg_rewind: true
        parameters:
    initdb:
    - encoding: UTF8
    - data-checksums
    pg_hba:
    - host replication replicator 10.128.0.0/8 md5
    - host all all 10.128.0.0/8 md5
    users:
      admin:
        password: admin321
        options:
          - createrole
          - createdb
    postgresql:
      listen: 127.0.0.1, 10.128.3.179:5432
      connect_address: 10.128.3.179:5432
      data_dir: /var/lib/postgresql/15/main
      bin_dir: /usr/lib/postgresql/15/bin
      pgpass: /tmp/pgpass0
      authentication:
        replication:
          username: replicator
          password: reppass321
        superuser:
        username: postgres
        password: zalando321
      rewind:  # Has no effect on postgres 10 and lower
        username: rewinduser
        password: rewindpassword321
    parameters:
      unix_socket_directories: '.'
  tags:
      nofailover: false
      noloadbalance: false
      clonefrom: false
      nosync: false

  -------------------------------------------------------------

  -- запускаем patroni
  sudo -u postgres patroni /etc/patroni.yml

  sudo systemctl is-enabled patroni 
  sudo systemctl enable patroni 
  sudo systemctl start patroni 
  sudo systemctl status patroni 

  -- посмотреть состояние patrony
  sudo patronictl -c /etc/patroni.yml list 


### Установка pgbouncer

  ---- выполняется на каждой ноде с patroni
  sudo apt install -y pgbouncer

  -- Создал базу chicago  
  create database chicago;

  sudo nano /etc/pgbouncer/pgbouncer.ini

  -- Вставляем в /etc/pgbouncer/pgbouncer.ini

  -----------------------------------------------
  [databases]
  chicago = host=127.0.0.1 port=5432 dbname=chicago
  postgres = host=127.0.0.1 port=5432 dbname=postgres 
  [pgbouncer]
  logfile = /var/log/postgresql/pgbouncer.log
  pidfile = /var/run/postgresql/pgbouncer.pid
  listen_addr = *
  listen_port = 6432
  auth_type = scram-sha-256
  auth_file = /etc/pgbouncer/userlist.txt
  admin_users = postgres

  ----------------------------------------------

  ---- Установил  net-tools посмотрим процессы
  sudo apt install net-tools
  netstat -pltn

  sudo systemctl status pgbouncer 

  sudo systemctl stop pgbouncer 

  sudo systemctl start pgbouncer

  -- in psql
  sudo -u postgres psql -h localhost

  create user admindb with password 'root123';
  select * from users;

  \du
  list role

  -- Необходимо внести информацию в /etc/pgbouncer/userlist.txt  о пользователях, подключающихся через pgbouncer

  sudo -u postgres psql -h localhost
  postgres=# select usename,passwd from pg_shadow;

  postgres=# select usename,passwd from pg_shadow;
    usename   |                                                                passwd
  ------------+---------------------------------------------------------------------------------------------------------------------------------------
   postgres   | SCRAM-SHA-256$4096:5cqMWyUp7NXnmN8zjV57Mw==$jYqzV+uObx/lQKoRqzcmtDjK6TbBWhTSoMwpBQtbO08=:+nkbPefAZRvba5DwYhpZJ0jkSv2Th9IbcpMG/w2XgJc=
   replicator | SCRAM-SHA-256$4096:+RCfcLr9LluaLG2erl3B1Q==$I5oLAwLn5CxTKt88iT12Dx4DmLS2dk779NKKDDVQFRw=:FeQwIJZb8w0I77JR5h508zwHQInZq/LM78s6lfSPBKc=
   rewinduser | SCRAM-SHA-256$4096:YNqjcnz48qUaF0A+8Cguqg==$LIVxp6hvl02jK3WThTATI/UsqSznzqleG1Dw0k4c2DQ=:Ttb1IhNXO1mlZO6WcnOdaKhl4mb0PeKow60qxR7QOWA=
   admin      | SCRAM-SHA-256$4096:hPerj68H0/d76WbyRvEMvw==$d2VRoWfaAdnLznXsT5M+owajHSS9tPyMK0ukjKkbcfk=:xdHQ6LAuc2qQO99tK2YFr+eT7ZijDrMcY8qGpLmp5cc=
   admindb    | SCRAM-SHA-256$4096:w4JSNbEvMXjD70G82kJArA==$TNclao7ZMdK4Z3X3CQYmMbF+1uuC1s1hW/gopgNNxGQ=:WpZLaNlVGsHslBswr/3TCqPkk58fOrSD0gbptdIKgXo=
  (5 rows)

   sudo nano /etc/pgbouncer/userlist.txt

  -- Добавляем записи

  "postgres"  "SCRAM-SHA-256$4096:Z7I9sYPjjLJiDGlEa2NGjg==$Xz1MZAcKJCF+dxJhRN9WcHaBt2WnsrLUa1PbwxVpiCU=:YmNHdjAARNXVYzeuEkRlHMPlc8LJsNTUXmcERVHz9y8="
  "testuser"  "SCRAM-SHA-256$4096:g0UGgROg54Ifbtv9eaOWpQ==$Fh6t0lwQJiDGd66EPDKmm7korJlUlv5c0zoIXuYaT6k=:sj4aLUYazRLJRP1dDltc36o6YQRISzI3r6GBQlDxZ74="

  -- рестарт pg_bouncer
  sudo systemctl restart pgbouncer 

  -- При падении сам стартует
  sudo nano /lib/systemd/system/pgbouncer.service
  -- добавляем строку:
  Restart=always


### Установка HAPROXY

  -- Порт HAPROXY 6432 vm-postgresql-4 10.128.183

  -- Выполнил на ноде vm-postgresql-4 10.128.3.183
  sudo apt install -y --no-install-recommends software-properties-common && sudo add-apt-repository -y ppa:vbernat/haproxy-2.5 && sudo apt install -y haproxy=2.5.\*

  -- Проверил состояния на ноде haproxy
  curl -v 10.128.3.183:8008/master

  -- Настроим /etc/haproxy/haproxy.cfg.
  -- Добавим записи в /etc/haproxy/haproxy.cfg 

  sudo nano /etc/haproxy/haproxy.cfg

  ---------------------------------------------------------------------------

  listen postgres_write
      bind *:5432
      mode            tcp
      option httpchk
      http-check connect
      http-check send meth GET uri /master
      http-check expect status 200
      default-server inter 10s fall 3 rise 3 on-marked-down shutdown-sessions
      server st-postgresql-dev-1.systtech.ru 10.128.3.134:6432 check port 8008
      server st-postgresql-dev-3.systtech.ru 10.128.3.179:6432 check port 8008

  listen postgres_read
      bind *:5433
      mode            tcp
      http-check connect
      http-check send meth GET uri /replica
      http-check expect status 200
      default-server inter 10s fall 3 rise 3 on-marked-down shutdown-sessions
      server vm-postgresql-1 10.128.3.134:6432 check port 8008
      server vm-postgresql-3 10.128.3.179:6432 check port 8008
  ---------------------------------------------------------------------------

  -- Выполняем рестарт haproxy.service и смотрим статус
  sudo systemctl restart haproxy.service
  sudo systemctl status haproxy.service

  -- Смотрим лог
  sudo cat /var/log/haproxy.log

### Настройка keepalived на хостах с HAproxy
  -- HAproxy устанавливается на ноде vm-postgresql-4 10.128.3.183

  sudo apt install -y keepalived

  -- Включаем виртуальный сетевой коммутатор
  sudo nano /etc/sysctl.conf

  -- дописываем
  net.ipv4.ip_nonlocal_bind=1

  sudo sysctl -p

  -- посмотрим на какой интерфейс нужно добавить виртуальный ip
  ip a

  sudo nano /etc/keepalived/keepalived.conf

  --------------------------------------------

  global_defs {
  # Keepalived process identifier
  lvs_id haproxy_DH
  }
  # Script used to check if HAProxy is running
  vrrp_script check_haproxy {
  script "killall -0 haproxy"
  interval 2
  weight 2
  }
  # Virtual interface
  # The priority specifies the order in which the assigned interface to take over in a failover
  vrrp_instance VI_01 {
  state MASTER
  interface eth0
  virtual_router_id 51
  priority 101
  # The virtual ip address shared between the two loadbalancers
  virtual_ipaddress {
  10.128.3.210
  }
  track_script {
  check_haproxy
  }
  }
-------------------------------------------

  --Виртуальный ip
  virtual_ipaddress {
  10.128.3.210
  }

  sudo service keepalived start

  -- посмотрим успешность добавления IP
  ip a

  !!! Подключение к postgresql leader через порт 5432
  !!! Подключение к postgresql replica через порт 5432

# Установка и тестирование pg_probackup

### Для организации резервного копирования и восстановления кластера PostgreSQL c pg_probackup 
### используем ещё одну виртуальную машину с ubuntu 22.04 LTS и PostgreSQL 15 10.128.3.178 st-postgres2
### с этой машины производится запуск pg_probackup, который через ssh подключается к  хосте агентов,
### которые используют ssh соединение как канал связи и транспорта. 
### Копии кластера PostgreSQL хранятся на st-postgres2.
### Тестирование создания бэкапов и восстановления проводилось на кластере patroni-1 (vm-postgresql-1,vm-postgresql-3)

  -- Создаем на st-postgres2 пользователя, по которым мы будем запускать pg_probackup
  adduser backup_user

  -- Подключаемся к st-postgres2 как backup_user
  ssh-copy-id postgres@vm-postgresql-1
  cp /home/backup_user/.ssh/id_rsa.pub /tmp/isa

  -- Устанавливаем pg_probackup на ВМ с которой будет запускаться бэкапирование и восстановление, а так же на ВМ кластера 
  sudo sh -c 'echo "deb [arch=amd64] https://repo.postgrespro.ru/pg_probackup/deb/ $(lsb_release -cs) main-$(lsb_release -cs)" > /etc/apt/sources.list.d/pg_probackup.list' && sudo wget -O - https://repo.postgrespro.ru/pg_probackup/keys/GPG-KEY-PG_PROBACKUP | sudo apt-key add - && sudo apt-get update
  sudo DEBIAN_FRONTEND=noninteractive apt install pg-probackup-15 pg-probackup-15-dbg postgresql-contrib postgresql-15-pg-checksums -y

  -- На st-postgres2 создаём алиас  и определяем переменную, содержащую путь до директории с резервными копиями
  --  (предварительно создав эту директорию)
  mkdir /home/backup_user/backup_db
  echo "BACKUP_PATH=/home/backup_user/backup_db">>~/.bash_profile
  echo "export BACKUP_PATH">>~/.bash_profile
  echo "alias pg_probackup='pg_probackup-15'">>~/.bash_profile

  -- подгружаем профиль

  . .bash_profile

  на st-postgres2 инициализируем каталог резервных копий и добавляем новый экземпляр:
  
  pg_probackup-15 init
  INFO: Backup catalog '/home/backup_user/backup_db' successfully initialized
  backup_user@st-postgres2:~/backup_db$ ls
  backups  wal

  -- Добавляем экземпляр PostgreSQL в каталог, даём ему имя dev1, указываем параметры подключения по ssh — хост,
  -- имя пользователя, и путь до PGDATA.

  backup_user@st-postgres2:~$ pg_probackup-15 add-instance --instance=dev1 --remote-host=st-postgresql-dev-1.systtech.ru --remote-user=postgres --pgdata=/var/lib/postgresql/15/main
  INFO: Instance 'dev1' successfully initialized

  -- Определим политику удержания резервных копий:
  -- Получившая политика сводится к следующему:
  --   необходимо удерживать все резервные копии младше 7 дней
  --   при этом кол-во полных резервный копий должно быть не меньше двух
  pg_probackup-15 set-config --instance dev1 --retention-window=7 --retention-redundancy=2

  -- Для создания резервных копий необходимо подключение к БД, поэтому создаём в кластере PostgreSQL базу,
  -- к которой будет происходить подключение для управления процессом резервного копирования (с точки зрения безопасности это лучше,
  -- чем подключаться к продуктовой БД), а также изменяем параметры базы данных

  postgresql@st-postgresql-dev-1:~$ psql -U postgres -h localhost -p 5432 -d postgres
  postgres=# create database backup_user;
  CREATE DATABASE

  -- создаём роль БД, от имени которой будет осуществляться управление процессом резервного копирования

  BEGIN;
  CREATE ROLE backup WITH LOGIN;
  GRANT USAGE ON SCHEMA pg_catalog TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.current_setting(text) TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.set_config(text, text, boolean) TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_is_in_recovery() TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_backup_start(text, boolean) TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_backup_stop(boolean) TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_create_restore_point(text) TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_switch_wal() TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_last_wal_replay_lsn() TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.txid_current() TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.txid_current_snapshot() TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.txid_snapshot_xmax(txid_snapshot) TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_control_checkpoint() TO backup;
  COMMIT;

  ALTER ROLE backup WITH REPLICATION;
  ALTER USER backup PASSWORD 'backup321';
  GRANT SELECT ON TABLE pg_catalog.pg_database TO backup;





  sudo nano /var/lib/postgresql/15/main/pg_hba.conf
  -- Добавляю следующие правила в pg_hba.conf:

    host    backup_user   backup          st-postgres2        md5
    host    replication   backup          st-postgres2        md5

  --На st-postgres2 в домашней директории пользователя backup_user cоздаём файл, в котором прописываем имя сервера или его ip-адрес, порт, имя базы,
  --имя пользователя и его пароль. Этот файл нужен для того, чтобы при создании резервной копии пароль вводился автоматически.

  backup_user@st-postgres2:~$ echo "st-postgresql-dev-1:5432:replication:backup:backup321">>~/.pgpass
  backup_user@st-postgres2:~$ echo "st-postgresql-dev-1:5432:backup_user:backup:backup321">>~/.pgpass

  -- Устанавливаю необходимые права на доступ к этому файлу

  chmod 600 ~/.pgpass

  -- Выполняю бэкап

  backup_user@st-postgres2:~$ pg_probackup-15  backup  --instance=dev1 -j2 --backup-mode=FULL --compress --stream --delete-expired --pguser=backup --pgdatabase=backup_user --remote-host=st-postgresql-dev-1.systtech.ru --remote-user=postgres
  INFO: Backup start, pg_probackup version: 2.5.12, instance: dev1, backup ID: RX2DAA, backup mode: FULL, wal mode: STREAM, remote: true, compress-algorithm: zlib, compress-level: 1
  INFO: This PostgreSQL instance was initialized with data block checksums. Data block corruption will be detected
  INFO: Database backup start
  INFO: wait for pg_backup_start()
  INFO: Wait for WAL segment /home/backup_user/backup_db/backups/dev1/RX2DAA/database/pg_wal/000000220000000000000014 to be streamed
  INFO: PGDATA size: 37MB
  INFO: Current Start LSN: 0/14000028, TLI: 22
  INFO: Start transferring data files
  INFO: Data files are transferred, time elapsed: 3s
  INFO: wait for pg_stop_backup()
  INFO: pg_stop backup() successfully executed
  INFO: stop_lsn: 0/140001A0
  INFO: Getting the Recovery Time from WAL
  INFO: Syncing backup files to disk
  INFO: Backup files are synced, time elapsed: 2s
  INFO: Validating backup RX2DAA
  INFO: Backup RX2DAA data files are valid
  INFO: Backup RX2DAA resident size: 28MB
  INFO: Backup RX2DAA completed
  INFO: Evaluate backups by retention
  INFO: Backup RX2DAA, mode: FULL, status: OK. Redundancy: 1/2, Time Window: 0d/7d. Active
  INFO: Backup RX2CZT, mode: FULL, status: ERROR. Redundancy: 2/2, Time Window: 0d/7d. Active
  INFO: Backup RX2CSE, mode: FULL, status: ERROR. Redundancy: 2/2, Time Window: 0d/7d. Active
  INFO: Backup RX26LW, mode: FULL, status: ERROR. Redundancy: 2/2, Time Window: 0d/7d. Active
  INFO: Backup RX1ZUS, mode: FULL, status: ERROR. Redundancy: 2/2, Time Window: 0d/7d. Active
  INFO: Backup RX1Z5W, mode: FULL, status: ERROR. Redundancy: 2/2, Time Window: 0d/7d. Active
  INFO: Backup RX19GI, mode: FULL, status: ERROR. Redundancy: 2/2, Time Window: 0d/7d. Active
  INFO: Backup RX19CE, mode: FULL, status: ERROR. Redundancy: 2/2, Time Window: 0d/7d. Active
  INFO: Backup RX198Q, mode: FULL, status: ERROR. Redundancy: 2/2, Time Window: 0d/7d. Active
  INFO: Backup RX191V, mode: FULL, status: ERROR. Redundancy: 2/2, Time Window: 0d/7d. Active
  INFO: Backup RX18WY, mode: FULL, status: ERROR. Redundancy: 2/2, Time Window: 0d/7d. Active
  INFO: There are no backups to merge by retention policy
  INFO: There are no backups to delete by retention policy
  INFO: There is no WAL to purge by retention policy

  -- Сокращаю команду создания бэкапов — прописываю часть параметров в конфигурацию экземпляра dev1:

  pg_probackup-15 set-config --instance=dev1 --remote-host=vm-postgresql-1.systtech.ru --remote-user=postgres --pguser=backup --pgdatabase=backup_user --log-filename=backup_cron.log --log-level-file=log --log-directory=/home/backup_user/backup_db/log

  -- Делаю инкрементальную копию в DELTA режиме, не указывая установленные в конфиге параметры:

  pg_probackup backup --instance=dev1 -j2 --progress -b DELTA --compress --stream --delete-expired

### Для того, чтобы появилась возможность восстановления на выбранный момент времени (точку восстановления), необходимо иметь резервную копию,
### сделанную до точки восстановления, а также все WAL файлы от момента начала резервной копии и до точки восстановления.

  postgresql@vm-postgresql-1:~$ sudo su postgres
  postgres@vm-postgresql-1:/home/postgresql$ ssh-keygen -t rsa
  postgres@vm-postgresql-1:/home/postgresql$ ssh-copy-id backup_user@st-postgres2
  postgres@vm-postgresql-1:/home/postgresql$  psql -U postgres -h localhost -p 5432 -d postgres
  psql (15.3 (Ubuntu 15.3-1.pgdg22.04+1))
  postgres=# alter system set archive_mode = on;
  postgres=# alter system set archive_command = 'pg_probackup-15 archive-push -B /home/backup_user/backup_db --instance=dev1 --wal-file-path=%p --wal-file-name=%f --remote-host=st-postgres2 --remote-user=backup_user --compress';

  postgresql@vm-postgresql-3:~$ sudo su postgres
  postgres@vm-postgresql-3:/home/postgresql$ ssh-keygen -t rsa
  postgres@vm-postgresql-3:/home/postgresql$ ssh-copy-id backup_user@st-postgres2
  postgres@vm-postgresql-3:/home/postgresql$  psql -U postgres -h localhost -p 5432 -d postgres
  psql (15.3 (Ubuntu 15.3-1.pgdg22.04+1))
  postgres=# alter system set archive_mode = on;
  postgres=# alter system set archive_command = 'pg_probackup-15 archive-push -B /home/backup_user/backup_db --instance=dev1 --wal-file-path=%p --wal-file-name=%f --remote-host=st-postgres2 --remote-user=backup_user --compress';

### В кластере бэкап целесообразно делать с ноды replica. В разные моменты времени одна и таже нода может быть нодой leader и нодой replica.
  --- Выполняю
  postgresql@vm-postgresql-3:~$ ssh-keygen -t rsa
  postgresql@vm-postgresql-3:~$ ssh-copy-id backup_user@st-postgres2
  backup_user@st-postgres2:sudo nano /var/lib/postgresql/15/main/pg_hba.conf

  vm-postgresql-3:5432:replication:backup:backup321
  vm-postgresql-3:5432:backupdb:backup:backup321


### Создадию скрипт bkp_base.sh, который будет контролировать, какая нода является leader, какая replica и запускать  резервное копирование
### с ноды replica

  # подгружаем профиль с настройками
  . /home/backup_user/.bash_profile
  if curl -v vm-postgresql-1:8008/replica 2>&1 | grep -q '200'; then pg_probackup set-config --instance dev1 --remote-host=vm-postgresql-1;
  elif curl -v st-postgresql-dev-3:8008/replica 2>&1 | grep -q '200'; then pg_probackup-15 set-config --instance dev1 --remote-host=vm-postgresql-3;
  elif curl -v st-postgresql-dev-1:8008/master 2>&1 | grep -q '200'; then pg_probackup-15 set-config --instance dev1 --remote-host=vm-postgresql-1;
  else pg_probackup-15 set-config --instance dev1 --remote-host=vm-postgresql-3;
  fi
  # вызываем команду создания копии, в качестве параметра передаем метод создания бэкапа (FULL, DELTA и т.д.)
  pg_probackup-15 backup --instance=dev1 -j 2 --progress -b $1 --stream  --compress --delete-expired --delete-wal

  chmod 777 bkp_base.sh

  -- Записываю в crontab вызов полученного скрипта, например, можно задать такое расписание:

  crontab -e

  00 01 * * 7 /home/backup_user/scr/bkp_base.sh FULL
  00 01 * * 1,2,3,4,5,6 /home/backup_user/scr/bkp_base.sh DELTA

# Восстановление кластера patroni из бэкапа 

  backup_user@st-postgres2:~$  pg_probackup-15 show

  BACKUP INSTANCE 'dev1'
  ==========================================================================================================================================
   Instance  Version  ID      Recovery Time           Mode   WAL Mode  TLI       Time    Data   WAL  Zratio  Start LSN   Stop LSN    Status
  ==========================================================================================================================================
   dev1      15       RZIG41  2023-08-17 01:00:10+00  DELTA  STREAM    48/48  10m:16s  9550kB  32MB    1.00  1/A7D1F268  1/A8000110  OK
   dev1      15       RZGLG1  2023-08-16 01:00:12+00  DELTA  STREAM    48/48  10m:24s   130kB  32MB    1.00  1/A7D1F268  1/A8000110  OK  
   dev1      15       RZEQS1  2023-08-15 01:00:11+00  DELTA  STREAM    48/48  10m:15s  9550kB  32MB    1.00  1/A7D1F268  1/A8000110  OK
   dev1      15       RZCW42  2023-08-14 01:00:10+00  DELTA  STREAM    48/48  10m:13s   130kB  32MB    1.00  1/A7D1F268  1/A8000110  OK
   dev1      15       RZB1G2  2023-08-13 01:00:35+00  FULL   STREAM    48/0   10m:45s   239MB  32MB    7.39  1/A7D1F268  1/A8000110  OK
   dev1      15       RZ96S1  2023-08-12 01:00:10+00  DELTA  STREAM    48/48  10m:14s  9550kB  32MB    1.00  1/A7D1F268  1/A8000110  OK
   dev1      15       RZ7C41  2023-08-11 01:00:10+00  DELTA  STREAM    48/48  10m:15s   130kB  32MB    1.00  1/A7D1F268  1/A8000110  OK
   dev1      15       RZ5HG1  2023-08-10 01:00:12+00  DELTA  STREAM    48/48  10m:24s  9550kB  32MB    1.00  1/A7D1F268  1/A8000110  OK
   dev1      15       RZ3MS1  2023-08-09 01:00:10+00  DELTA  STREAM    48/48  10m:14s   130kB  32MB    1.00  1/A7D1F268  1/A8000110  OK
   dev1      15       RZ1S41  2023-08-08 01:00:10+00  DELTA  STREAM    48/48  10m:15s  9550kB  32MB    1.00  1/A7D1F268  1/A8000110  OK
   dev1      15       RYZXG1  2023-08-07 01:00:10+00  DELTA  STREAM    48/48  10m:16s   130kB  32MB    1.00  1/A7D1F268  1/A8000110  OK
   dev1      15       RYY2S1  2023-08-06 01:00:34+00  FULL   STREAM    48/0   10m:46s   239MB  32MB    7.39  1/A7D1F268  1/A8000110  OK

  ---- Восстанавливю на vm-postgresql-1
  backup_user@st-postgres2:~$ pg_probackup set-config --instance dev1 --remote-host=vm-postgresql-1

  postgresql@vm-postgresql-1:~$ sudo patronictl -c /etc/patroni.yml list
  + Cluster: patroni-lite ----+---------+---------+----+-----------+
  | Member    | Host          | Role    | State   | TL | Lag in MB |
  +-----------+---------------+---------+---------+----+-----------+
  | patroni-1 | 192.168.3.134 | Leader  | running | 48 |           |
  | patroni-2 | 192.168.3.179 | Replica | running | 48 |         0 |
  +-----------+---------------+---------+---------+----+-----------+

  postgresql@vm-postgresql-3:~$ sudo systemctl stop patroni
  postgresql@vm-postgresql-1:~$ sudo systemctl stop patroni


  postgresql@vm-postgresql-1:~$ sudo su postgres
  postgres@vm-postgresql-1:/home/postgresql$ rm -rf /var/lib/postgresql/15/main*

  --- Восстанавливаю на ID RZCW42 

  backup_user@st-postgres2:~$ pg_probackup restore -B /home/backup_user/backup_db --instance dev1 -i RZCW42
  INFO: Validating parents for backup RZCW42
  INFO: Validating backup RZB1G2
  INFO: Backup RZB1G2 data files are valid
  INFO: Validating backup RZCW42
  INFO: Backup RZCW42 data files are valid
  INFO: Backup RZCW42 WAL segments are valid
  INFO: Backup RZCW42 is valid.
  INFO: Restoring the database from backup at 2023-08-14 01:00:02+00
  INFO: Start restoring backup files. PGDATA size: 1802MB
  INFO: Backup files are restored. Transfered bytes: 1802MB, time elapsed: 20s
  INFO: Restore incremental ratio (less is better): 100% (1802MB/1802MB)
  INFO: Syncing restored files to disk
  INFO: Restored backup files are synced, time elapsed: 7s
  INFO: Restore of backup RZCW42 completed.

  -- Выполняю
  postgres@st-postgresql-dev-1:/home/postgresql$ patronictl -c /etc/patroni.yml remove patroni-lite
  + Cluster: patroni-lite -------+----+-----------+
  | Member | Host | Role | State | TL | Lag in MB |
  +--------+------+------+-------+----+-----------+
  +--------+------+------+-------+----+-----------+
  Please confirm the cluster name to remove: patroni-lite
  You are about to remove all information in DCS for patroni-lite, please type: "Yes I am aware": Yes I am aware

  postgres@vm-postgresql-1:/home/postgresql$ service patroni start
  ==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ===
  Authentication is required to start 'patroni.service'.
  Authenticating as: postgresql
  Password:
  ==== AUTHENTICATION COMPLETE ===
  postgresql@vm-postgresql-1:~$ sudo patronictl -c /etc/patroni.yml list
  + Cluster: patroni-lite ----+--------+---------+----+-----------+
  | Member    | Host          | Role   | State   | TL | Lag in MB |
  +-----------+---------------+--------+---------+----+-----------+
  | patroni-1 | 192.168.3.134 | Leader | running | 49 |           |
  +-----------+---------------+--------+---------+----+-----------+

  postgresql@vm-postgresql-3:~$ service patroni start
  ==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ===
  Authentication is required to start 'patroni.service'.
  Authenticating as: postgresql
  Password:
  ==== AUTHENTICATION COMPLETE ===

  postgresql@vm-postgresql-1:~$ sudo patronictl -c /etc/patroni.yml list
  + Cluster: patroni-lite ----+---------+---------+----+-----------+
  | Member    | Host          | Role    | State   | TL | Lag in MB |
  +-----------+---------------+---------+---------+----+-----------+
  | patroni-1 | 192.168.3.134 | Leader  | running | 49 |           |
  | patroni-2 | 192.168.3.179 | Replica | running | 48 |         0 |
  +-----------+---------------+---------+---------+----+-----------+

  -- Через некоторое время

  postgresql@vm-postgresql-1:~$ sudo patronictl -c /etc/patroni.yml list
  + Cluster: patroni-lite ----+---------+---------+----+-----------+
  | Member    | Host          | Role    | State   | TL | Lag in MB |
  +-----------+---------------+---------+---------+----+-----------+
  | patroni-1 | 192.168.3.134 | Leader  | running | 49 |           |
  | patroni-2 | 192.168.3.179 | Replica | running | 49 |         0 |
  +-----------+---------------+---------+---------+----+-----------+

# Восстановление проведено успешно!!!

# ЧАСТИЧНОЕ ВОССТАНОВЛЕНИЕ
### Восстанавливаем тольк базу данных chicago

  postgres@vm-postgresql-1:/home/postgresql$ rm -rf /var/lib/postgresql/15/main*
  postgres@vm-postgresql-3:/home/postgresql$ rm -rf /var/lib/postgresql/15/main*
  backup_user@st-postgres2:~/scr$ pg_probackup restore -B /home/backup_user/backup_db --instance dev1  --db-include=chicago  -i RZO041
  INFO: Validating backup RZO041
  INFO: Backup RZO041 data files are valid
  INFO: Backup RZO041 WAL segments are valid
  INFO: Backup RZO041 is valid.
  INFO: Restoring the database from backup at 2023-08-20 01:00:01+00
  INFO: Start restoring backup files. PGDATA size: 1824MB
  INFO: Backup files are restored. Transfered bytes: 1508MB, time elapsed: 15s
  INFO: Restore incremental ratio (less is better): 83% (1508MB/1824MB)
  INFO: Syncing restored files to disk
  INFO: Restored backup files are synced, time elapsed: 6s
  INFO: Restore of backup RZO041 completed.
  postgresql@vm-postgresql-1:~$ patronictl -c /etc/patroni.yml remove patroni-lite
  + Cluster: patroni-lite -------+----+-----------+
  | Member | Host | Role | State | TL | Lag in MB |
  +--------+------+------+-------+----+-----------+
  +--------+------+------+-------+----+-----------+
  Please confirm the cluster name to remove: patroni-lite
  You are about to remove all information in DCS for patroni-lite, please type: "Yes I am aware": Yes I am aware
  postgresql@vm-postgresql-1:~$ sudo patronictl -c /etc/patroni.yml list
  + Cluster: patroni-lite -------+----+-----------+
  | Member | Host | Role | State | TL | Lag in MB |
  +--------+------+------+-------+----+-----------+
  +--------+------+------+-------+----+-----------+
  postgresql@vm-postgresql-1:~$ sudo patronictl -c /etc/patroni.yml list
  + Cluster: patroni-lite ----+---------+---------+----+-----------+
  | Member    | Host          | Role    | State   | TL | Lag in MB |
  +-----------+---------------+---------+---------+----+-----------+
  | patroni-1 | 192.168.3.134 | Replica | running |    |   unknown |
  +-----------+---------------+---------+---------+----+-----------+

###!!!!! НОДА НЕ ПЕРЕХОДИТ В РЕЖИМ leader  !!!!!

  postgresql@st-postgresql-dev-3:~$ service patroni start
  ==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ===
  Authentication is required to start 'patroni.service'.
  Authenticating as: postgresql
  Password:
  postgresql@vm-postgresql-1:~$ sudo patronictl -c /etc/patroni.yml list
  + Cluster: patroni-lite ----+---------+---------+----+-----------+
  | Member    | Host          | Role    | State   | TL | Lag in MB |
  +-----------+---------------+---------+---------+----+-----------+
  | patroni-1 | 192.168.3.134 | Replica | running |    |   unknown |
  | patroni-2 | 192.168.3.179 | Replica | stopped |    |   unknown |
  +-----------+---------------+---------+---------+----+-----------+

#!!!!! ПРОБУЕМ СДЕЛАТЬ ЧАСТИЧНОЕ ВОССТАНОВЛЕНИЕ НА ДРУГОЙ КЛАСТЕР

  postgresql@vm-postgresql-1:~$ sudo pg_createcluster 15 main2
  postgresql@vm-postgresql-1:~$ pg_lsclusters
  Ver Cluster Port Status Owner     Data directory               Log file
  15  main2   5433 down   <unknown> /var/lib/postgresql/15/main2 /var/log/postgresql/postgresql-15-main2.log

  sudo pg_ctlcluster 15 main2 start

  sudo apt install postgresql postgresql-client -y
  sudo sh -c 'echo "listen_addresses = '"'*'"'" >> /etc/postgresql/15/main2/postgresql.conf'
  sudo sh -c 'echo "host    all             all             0.0.0.0/0              scram-sha-256" >> /etc/postgresql/15/main2/pg_hba.conf'
  sudo sed -i 's|host    all             all             127.0.0.1/32            scram-sha-256|host    all             all             127.0.0.1/32            trust|g' /etc/postgresql/15/main2/pg_hba.conf
  sudo systemctl restart postgresql.service

  postgresql@st-postgresql-dev-1:~$ sudo nano /etc/postgresql/15/main2/postgresql.conf

  listen_addresses = '*'          # what IP address(es) to listen on;
                                          # comma-separated list of addresses;
                                          # defaults to 'localhost'; use '*' for all
                                          # (change requires restart)
  port = 5435                             # (change requires restart)
  max_connections = 100                   # (change requires restart)
  #superuser_reserved_connections = 3     # (change requires restart)
  unix_socket_directories = '/var/run/postgresql' # comma-separated list of directories
                                          # (change requires restart)
  #unix_socket_group = ''                 # (change requires restart)
  #unix_socket_permissions = 0777         # begin with 0 to use octal notation
                                          # (change requires restart)
  #bonjour = off                          # advertise server via Bonjour
                                          # (change requires restart)
  #bonjour_name = ''                      # defaults to the computer name
                                        # (change requires restart)


  sudo psql -U postgres -h 127.0.0.1 -p 5435 -c "alter user postgres with password 'zalando321';"

  sudo psql -U postgres -h st-postgresql-dev-1 -p 5435 -d postgres

  postgresql@vm-postgresql-1:~$ sudo psql -U postgres -h 127.0.1.1 -p 5435 -d postgres

  sudo nano /etc/postgresql/15/main2/pg_hba.conf

  sudo sh -c 'echo "listen_addresses = '"'*'"'" >> /etc/postgresql/15/main2/postgresql.conf'

  locate pg_hba.conf

  postgresql@st-postgresql-dev-1:~$ sudo apt install plocate

  postgresql@st-postgresql-dev-1:~$ locate pg_hba.conf
  /etc/postgresql/15/main2/pg_hba.conf
  /usr/share/postgresql/15/pg_hba.conf.sample
  sudo nano /etc/postgresql/15/main2/pg_hba.conf

  local all postgres trust

  psql -U postgres

  alter user postgres with  password 'zalando321';


  postgresql@st-postgresql-dev-1:~$ psql -U postgres -h localhost -p 5435 -d postgres

  postgres=# create database backup_user
  postgres=# create database backup_user;
  CREATE DATABASE


  BEGIN;
  CREATE ROLE backup WITH LOGIN;
  GRANT USAGE ON SCHEMA pg_catalog TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.current_setting(text) TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.set_config(text, text, boolean) TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_is_in_recovery() TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_backup_start(text, boolean) TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_backup_stop(boolean) TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_create_restore_point(text) TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_switch_wal() TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_last_wal_replay_lsn() TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.txid_current() TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.txid_current_snapshot() TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.txid_snapshot_xmax(txid_snapshot) TO backup;
  GRANT EXECUTE ON FUNCTION pg_catalog.pg_control_checkpoint() TO backup;
  COMMIT;

  ALTER ROLE backup WITH REPLICATION;

  ALTER USER backup PASSWORD 'backup321';

  backup_user@st-postgres2:~$ pg_probackup-15 show


  BACKUP INSTANCE 'dev1'
  =======================================================================================================================================
   Instance  Version  ID      Recovery Time           Mode   WAL Mode  TLI    Time    Data   WAL  Zratio  Start LSN   Stop LSN    Status
  =======================================================================================================================================
   dev1      15       S0JHG2  2023-09-06 01:00:11+00  DELTA  STREAM    52/52   11s   131kB  16MB    1.00  1/F3000028  1/F30001A0  OK
   dev1      15       S0HMS1  2023-09-05 01:00:14+00  DELTA  STREAM    52/52   24s  9762kB  16MB    1.00  1/F1000028  1/F10001A0  OK
   dev1      15       S0FS41  2023-09-04 01:00:09+00  DELTA  STREAM    52/52   11s   131kB  16MB    1.00  1/EF000028  1/EF0001A0  OK
   dev1      15       S0DXG1  2023-09-03 01:00:40+00  FULL   STREAM    52/0    44s   246MB  32MB    7.23  1/ED000028  1/ED0001A0  OK
   dev1      15       S0C2S1  2023-09-02 01:00:14+00  DELTA  STREAM    51/51   14s   131kB  16MB    1.00  1/EA000028  1/EA0001A0  OK
   dev1      15       S0A842  2023-09-01 01:00:10+00  DELTA  STREAM    51/51   12s  9762kB  16MB    1.00  1/E8000028  1/E80001A0  OK
   dev1      15       S08DG1  2023-08-31 01:00:11+00  DELTA  STREAM    51/51   12s   131kB  32MB    1.00  1/E6000028  1/E6000168  OK
   dev1      15       S06IS1  2023-08-30 01:00:10+00  DELTA  STREAM    51/51   13s  9762kB  16MB    1.00  1/E4000028  1/E40001A0  OK
   dev1      15       S04O41  2023-08-29 01:00:15+00  DELTA  STREAM    51/51   23s   131kB  16MB    1.00  1/E2000028  1/E20001A0  OK
   dev1      15       S02TG1  2023-08-28 01:00:09+00  DELTA  STREAM    51/50   12s  9762kB  16MB    1.00  1/E0000028  1/E00001A0  OK
   dev1      15       RZU121  2023-08-23 07:06:11+00  FULL   STREAM    50/0    14s    63MB  16MB    5.35  2/140000D8  2/14007FB0  OK
   dev1      15       RZO041  2023-08-20 01:00:33+00  FULL   STREAM    50/0    37s   246MB  48MB    7.23  1/DA000028  1/DB0000B8  OK



  postgres@vm-postgresql-1:/home/postgresql$ rm -rf /var/lib/postgresql/15/main2*
  backup_user@st-postgres2:~$ pg_probackup restore --instance=dev1 -D /var/lib/postgresql/15/main2 -i S0DXG1 -j 2 --recovery-target='immediate'  --progress --remote-proto=ssh --remote-host=st-postgresql-dev-1.systtech.ru  --archive-host=st-postgres2.systtech.ru --archive-user=backup --log-level-console=log --log-level-file=verbose --log-filename=restore_imm.log
  postgresql@st-postgresql-dev-1:~$ sudo pg_ctlcluster 15 main2 start

  postgresql@st-postgresql-dev-1:~$ sudo psql -U postgres -h 127.0.1.1 -p 5435 -d postgres
  postgres=# \l
                                                  List of databases
      Name     |  Owner   | Encoding |   Collate   |    Ctype    | ICU Locale | Locale Provider |   Access privileges
  -------------+----------+----------+-------------+-------------+------------+-----------------+-----------------------
   backup_user | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
   backuptest1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
   chicago     | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
   portal      | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
   postgres    | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
   template0   | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
               |          |          |             |             |            |                 | postgres=CTc/postgres
   template1   | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
               |          |          |             |             |            |                 | postgres=CTc/postgres
  postgres= # SELECT pg_size_pretty(pg_database_size('chicago'));
   pg_size_pretty
  ----------------
   1445 MB

  postgres=# SELECT pg_size_pretty(pg_database_size('portal'));
   pg_size_pretty
  ----------------

### Поднялась одна база. Остальные пустые. Как и описано в докеументации.

# Тестирование кластеров
  Тестирование компонентов кластера производилось отключением компонентов кластера под нагрузкой. 
  Использовалась программа pgbench. 
  pgbench запускалась с ноды  st-postgres2 10.128.3.178
  ВСЕ ПОДКЛЮЧЕНИЯ ДЛЯ ТЕСТИРОВАНИЯ ЧЕРЕЗ ВИРТУАЛЬНЫЙ ip 192.168.3.230  порт 5432 для кластера patroni 
  ip 192.168.3.210  порт 5432 для кластера patroni-lite

  postgresql@st-postgres2:~$ sudo pgbench -h192.168.3.230 -p5432 -Upostgres -i -s 15 chicago
  postgresql@st-postgres2:~$ sudo -h 192.168.3.230 -p 5432 -iu postgres pgbench -c 50 -j 2 -P 20 -T 980 chicago

### При тестировании отказоустойчивости кластеров столкнулся с интересной ситуацией: в случае остановки одной из нод при её
### запуске и наличии Lag in MB более 1000 нода реплика после включения не может догнать leader.
### Проблема решается реиницилизацией ноды patronictl -c /etc/patroni.yml reinit patroni vm-postgresql-dev-9.
### Данная ситуация проявляется на кластере patroni и не проявляется на кластере patroni-lite/


